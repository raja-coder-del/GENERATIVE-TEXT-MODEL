{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCGztyfvPq41n2IiPi5RMx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raja-coder-del/GENERATIVE-TEXT-MODEL/blob/main/task_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "\n",
        "# --- 1. Setup and Imports ---\n",
        "print(\"--- 1. Setting up environment and importing libraries ---\")\n",
        "# Install transformers and datasets if not already installed\n",
        "# !pip install transformers datasets\n",
        "\n",
        "# Check if GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 2. Dataset Preparation ---\n",
        "print(\"\\n--- 2. Preparing the dataset ---\")\n",
        "\n",
        "# Create a dummy text file for demonstration.\n",
        "# In a real scenario, you would replace this with your actual, larger dataset.\n",
        "# The content here is just to give the model some context, even if minimal.\n",
        "text_data = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog. This is a classic sentence for testing.\n",
        "Artificial intelligence is a rapidly evolving field. It has the potential to transform many aspects of our lives.\n",
        "Machine learning is a subset of AI that focuses on algorithms that learn from data. Deep learning is a further subset of machine learning.\n",
        "The future of technology is exciting, with new innovations emerging constantly. We are living in a truly remarkable era of advancements.\n",
        "Climate change is a pressing global issue that requires urgent action. Sustainable solutions are crucial for our planet's future.\n",
        "Generative AI models are revolutionizing creative industries and content creation.\n",
        "Large Language Models (LLMs) like GPT are capable of understanding and generating human-like text.\n",
        "Natural Language Processing (NLP) is a branch of AI that deals with the interaction between computers and human language.\n",
        "The history of computing is filled with groundbreaking discoveries, from early calculating machines to modern supercomputers.\n",
        "Space exploration continues to push the boundaries of human knowledge and our understanding of the universe.\n",
        "\"\"\"\n",
        "\n",
        "file_path = \"my_gpt_corpus.txt\"\n",
        "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text_data)\n",
        "\n",
        "print(f\"Dummy text data saved to: {file_path}\")\n",
        "\n",
        "# Load the text data\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "# Basic cleaning of the text data\n",
        "cleaned_text = raw_text.lower()\n",
        "# Replace multiple spaces/newlines/tabs with a single space\n",
        "cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "print(f\"Corpus length after cleaning: {len(cleaned_text)} characters\")\n",
        "\n",
        "# --- 3. Model Building and (Conceptual) Fine-tuning ---\n",
        "print(\"\\n--- 3. Loading GPT-2 model and tokenizer ---\")\n",
        "\n",
        "# Load pre-trained GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set pad_token_id for the tokenizer. GPT-2 tokenizer doesn't have a default pad token.\n",
        "# We'll set it to the EOS (End of Sequence) token for simplicity, which is common.\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(f\"Tokenizer pad_token set to eos_token: {tokenizer.pad_token}\")\n",
        "\n",
        "# Move model to the appropriate device (GPU if available)\n",
        "model.to(device)\n",
        "print(f\"Model moved to {device}\")\n",
        "\n",
        "# --- Conceptual Fine-tuning (Simplified for demonstration) ---\n",
        "# For actual fine-tuning, you would use `TextDataset`, `DataCollatorForLanguageModeling`,\n",
        "# and `Trainer` from the Hugging Face `transformers` library.\n",
        "# This part is just to illustrate that fine-tuning is a step, but for this small\n",
        "# dummy dataset, the impact on generation will be minimal compared to the base GPT-2.\n",
        "\n",
        "# Example of how you would prepare data for actual fine-tuning (not executed as a full training loop here):\n",
        "# from transformers import TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "#\n",
        "# # Create a TextDataset from your corpus\n",
        "# print(\"Creating TextDataset (conceptual for fine-tuning)...\")\n",
        "# dataset_for_training = TextDataset(\n",
        "#     tokenizer=tokenizer,\n",
        "#     file_path=file_path,\n",
        "#     block_size=128 # The maximum sequence length for input to the model\n",
        "# )\n",
        "#\n",
        "# # Data collator for language modeling (will handle padding and masking)\n",
        "# data_collator = DataCollatorForLanguageModeling(\n",
        "#     tokenizer=tokenizer, mlm=False # mlm=False for causal language modeling (GPT-style)\n",
        "# )\n",
        "#\n",
        "# # Define training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./gpt2_fine_tuned_model\", # Directory to save checkpoints\n",
        "#     overwrite_output_dir=True,\n",
        "#     num_train_epochs=3, # Number of training epochs\n",
        "#     per_device_train_batch_size=2, # Batch size per device during training\n",
        "#     save_steps=10_000, # Save checkpoint every X steps\n",
        "#     save_total_limit=2, # Limit the total number of checkpoints\n",
        "#     logging_dir=\"./logs\", # Directory for storing logs\n",
        "#     logging_steps=500,\n",
        "# )\n",
        "#\n",
        "# # Create a Trainer instance\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     data_collator=data_collator,\n",
        "#     train_dataset=dataset_for_training,\n",
        "# )\n",
        "#\n",
        "# print(\"Starting conceptual fine-tuning... (This is a simplified demonstration, not full training)\")\n",
        "# # To actually fine-tune, uncomment the line below. This will train the model.\n",
        "# # trainer.train()\n",
        "# print(\"Conceptual fine-tuning completed. (Model weights are not significantly updated with this small data)\")\n",
        "\n",
        "print(\"GPT-2 model loaded. For optimal results on specific topics, fine-tuning on a larger, domain-specific dataset is highly recommended.\")\n",
        "print(\"Proceeding with text generation using the base GPT-2 model (or minimally 'tuned' if you uncommented trainer.train()).\")\n",
        "\n",
        "# --- 4. Text Generation Function ---\n",
        "print(\"\\n--- 4. Defining the text generation function ---\")\n",
        "\n",
        "def generate_text_gpt2(prompt, model, tokenizer, max_length=100, temperature=0.7, top_k=50, top_p=0.9, num_return_sequences=1):\n",
        "    \"\"\"\n",
        "    Generates text using a GPT-2 model.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The starting text for generation.\n",
        "        model: The GPT2LMHeadModel instance.\n",
        "        tokenizer: The GPT2Tokenizer instance.\n",
        "        max_length (int): Maximum total length of the generated text (prompt + new text).\n",
        "        temperature (float): Controls randomness. Lower values (e.g., 0.5) make output more predictable/conservative.\n",
        "                             Higher values (e.g., 1.0+) make output more creative/diverse.\n",
        "        top_k (int): Limits sampling to the top-k most probable words. Set to 0 to disable.\n",
        "        top_p (float): Nucleus sampling: limits to tokens with cumulative probability top_p.\n",
        "                       Often used instead of or in conjunction with top_k.\n",
        "        num_return_sequences (int): Number of independent sequences to generate for the given prompt.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of generated text strings.\n",
        "    \"\"\"\n",
        "    # Encode the prompt text into input IDs\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Move input_ids to the same device as the model (GPU if available)\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    print(f\"Generating text for prompt: '{prompt}' with max_length={max_length}, temp={temperature}, top_k={top_k}, top_p={top_p}\")\n",
        "\n",
        "    # Generate text using the model's generate method\n",
        "    # This method is highly optimized and offers various decoding strategies.\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=max_length, # max_length includes the prompt tokens\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=1.2, # Discourage repeating the same words/phrases\n",
        "        do_sample=True, # Enable sampling (vs. greedy decoding or beam search)\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        pad_token_id=tokenizer.pad_token_id, # Essential for handling padding\n",
        "        eos_token_id=tokenizer.eos_token_id, # Model stops when EOS token is generated\n",
        "    )\n",
        "\n",
        "    generated_texts = []\n",
        "    for sequence in output_sequences:\n",
        "        # Decode the generated sequence back to text, skipping special tokens\n",
        "        text = tokenizer.decode(sequence, skip_special_tokens=True)\n",
        "\n",
        "        # GPT-2's generate method returns the prompt + generated text.\n",
        "        # We might want to remove the exact prompt from the beginning of the generated text\n",
        "        # to see only the newly generated part, or keep it to see the full output.\n",
        "        # Here, we'll keep the full output as it demonstrates coherence.\n",
        "        generated_texts.append(text.strip())\n",
        "\n",
        "    return generated_texts\n",
        "\n",
        "# --- 5. Demonstration with User Prompts ---\n",
        "print(\"\\n--- 5. Demonstrating Text Generation with User Prompts ---\")\n",
        "\n",
        "# Define a list of prompts related to various topics\n",
        "prompts = [\n",
        "    \"The future of artificial intelligence will be\",\n",
        "    \"Sustainable solutions for energy are essential because\",\n",
        "    \"Once upon a time, in a land far away, there lived\",\n",
        "    \"The latest scientific discovery indicates that\",\n",
        "    \"Generative AI models are transforming\",\n",
        "    \"Climate change impacts all of us, therefore we must\",\n",
        "    \"In the realm of natural language processing,\"\n",
        "]\n",
        "\n",
        "# Iterate through prompts and generate text\n",
        "for i, prompt in enumerate(prompts):\n",
        "    print(f\"\\n===== PROMPT {i+1} =====\")\n",
        "    print(f\"User Prompt: '{prompt}'\")\n",
        "\n",
        "    # Generate text with default parameters\n",
        "    generated_default = generate_text_gpt2(prompt, model, tokenizer,\n",
        "                                           max_length=80, # Generate up to 80 tokens in total\n",
        "                                           temperature=0.7,\n",
        "                                           top_k=50,\n",
        "                                           top_p=0.9,\n",
        "                                           num_return_sequences=1)\n",
        "    print(\"\\n--- Generated (Default Params) ---\")\n",
        "    print(generated_default[0])\n",
        "\n",
        "    # Experiment with different parameters for a single prompt\n",
        "    if i == 0: # Only for the first prompt to keep output manageable\n",
        "        print(\"\\n--- Experimenting with different parameters for the first prompt ---\")\n",
        "        print(\"\\n--- High Temperature (More Creative/Random) ---\")\n",
        "        generated_high_temp = generate_text_gpt2(prompt, model, tokenizer,\n",
        "                                                 max_length=80,\n",
        "                                                 temperature=1.0, # Higher temperature\n",
        "                                                 top_k=0, top_p=0.9, # Use only top_p\n",
        "                                                 num_return_sequences=1)\n",
        "        print(generated_high_temp[0])\n",
        "\n",
        "        print(\"\\n--- Low Temperature (More Conservative/Coherent) ---\")\n",
        "        generated_low_temp = generate_text_gpt2(prompt, model, tokenizer,\n",
        "                                                max_length=80,\n",
        "                                                temperature=0.5, # Lower temperature\n",
        "                                                top_k=50, top_p=0.9,\n",
        "                                                num_return_sequences=1)\n",
        "        print(generated_low_temp[0])\n",
        "\n",
        "        print(\"\\n--- Generating Multiple Sequences ---\")\n",
        "        generated_multiple = generate_text_gpt2(prompt, model, tokenizer,\n",
        "                                                max_length=60,\n",
        "                                                temperature=0.8,\n",
        "                                                top_k=50, top_p=0.9,\n",
        "                                                num_return_sequences=3) # Generate 3 sequences\n",
        "        for j, text in enumerate(generated_multiple):\n",
        "            print(f\"Sequence {j+1}:\\n{text}\\n\")\n",
        "\n",
        "print(\"\\n--- GPT-2 Text Generation Demo Complete ---\")\n",
        "\n",
        "# Optional: Clean up the dummy file\n",
        "# os.remove(file_path)\n",
        "# print(f\"Cleaned up dummy file: {file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_Q8T2J29N-M",
        "outputId": "8a1c398d-819b-478c-f741-0b3b3fb79ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Setting up environment and importing libraries ---\n",
            "Using device: cpu\n",
            "\n",
            "--- 2. Preparing the dataset ---\n",
            "Dummy text data saved to: my_gpt_corpus.txt\n",
            "Corpus length after cleaning: 1143 characters\n",
            "\n",
            "--- 3. Loading GPT-2 model and tokenizer ---\n",
            "Tokenizer pad_token set to eos_token: <|endoftext|>\n",
            "Model moved to cpu\n",
            "GPT-2 model loaded. For optimal results on specific topics, fine-tuning on a larger, domain-specific dataset is highly recommended.\n",
            "Proceeding with text generation using the base GPT-2 model (or minimally 'tuned' if you uncommented trainer.train()).\n",
            "\n",
            "--- 4. Defining the text generation function ---\n",
            "\n",
            "--- 5. Demonstrating Text Generation with User Prompts ---\n",
            "\n",
            "===== PROMPT 1 =====\n",
            "User Prompt: 'The future of artificial intelligence will be'\n",
            "Generating text for prompt: 'The future of artificial intelligence will be' with max_length=80, temp=0.7, top_k=50, top_p=0.9\n",
            "\n",
            "--- Generated (Default Params) ---\n",
            "The future of artificial intelligence will be decided by the technology itself.\n",
            "\n",
            "--- Experimenting with different parameters for the first prompt ---\n",
            "\n",
            "--- High Temperature (More Creative/Random) ---\n",
            "Generating text for prompt: 'The future of artificial intelligence will be' with max_length=80, temp=1.0, top_k=0, top_p=0.9\n",
            "The future of artificial intelligence will be the mystery when more algorithms may catch up to humans.\n",
            "\n",
            "--- Low Temperature (More Conservative/Coherent) ---\n",
            "Generating text for prompt: 'The future of artificial intelligence will be' with max_length=80, temp=0.5, top_k=50, top_p=0.9\n",
            "The future of artificial intelligence will be determined by the ability to create and communicate with people, not how they interact.\n",
            "\"I think we're going in a very good direction,\" said Mr. Gartner during his presentation at The International Conference on Artificial Intelligence (ICAI) held this week outside Stanford University's Computer Science & Engineering Institute. \"We'll see if there is any progress or I\n",
            "\n",
            "--- Generating Multiple Sequences ---\n",
            "Generating text for prompt: 'The future of artificial intelligence will be' with max_length=60, temp=0.8, top_k=50, top_p=0.9\n",
            "Sequence 1:\n",
            "The future of artificial intelligence will be determined by how the technology evolves and which technologies are being developed, but it is also possible to imagine a world where intelligent machines would become more ubiquitous.\n",
            "\n",
            "Sequence 2:\n",
            "The future of artificial intelligence will be driven by the ever more important question: How do we make good decisions?\n",
            ". . . [I]f AI becomes fully ubiquitous and widely accessible, then a significant part (if not all) of human life — in every facet except for its jobs — is\n",
            "\n",
            "Sequence 3:\n",
            "The future of artificial intelligence will be very bright indeed. It could take decades to build an AI that can learn, and then develop a complete human-like machine with the same level control over every part of our lives as we do now—and in some cases this may just never happen!\n",
            " (\n",
            "\n",
            "\n",
            "===== PROMPT 2 =====\n",
            "User Prompt: 'Sustainable solutions for energy are essential because'\n",
            "Generating text for prompt: 'Sustainable solutions for energy are essential because' with max_length=80, temp=0.7, top_k=50, top_p=0.9\n"
          ]
        }
      ]
    }
  ]
}